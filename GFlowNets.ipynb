{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM0bcdlewTeC4i72btoO85D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Importing libraries"],"metadata":{"id":"Ie3XXD940IX0"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np"],"metadata":{"id":"BcTa2ThfbWXb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defining the environment\n","The particle starts at $(0, 0)$. At each timestep, the particle will move up-right, right, or down-right. The sequence terminates after 5 timesteps, and the reward is equal to the absolute $y$ value of the ending position. The goal is to sample trajectories from $p(\\tau)$ proportional to the reward $R(\\tau)$, where $\\tau$ represents a trajectory or a sequence of points $(s_0 \\to s_1 \\to \\ldots \\to s_5)$."],"metadata":{"id":"cmu36PiObV81"}},{"cell_type":"code","source":["class GridEnvironment:\n","    def __init__(self, max_steps):\n","        self.max_steps = max_steps\n","        self.reset()\n","\n","    def reset(self):\n","        self.current_state = (0, 0)\n","        self.step_count = 0\n","        return self.current_state\n","\n","    def step(self, action):\n","        x, y = self.current_state\n","        x += 1\n","        y += action - 1  # -1, 0, or 1\n","        self.current_state = (x, y)\n","        self.step_count += 1\n","        done = self.step_count >= self.max_steps\n","        reward = max(abs(y), 0.1) if done else 0\n","        return self.current_state, reward, done"],"metadata":{"id":"vT2mIaGDlC5C","executionInfo":{"status":"ok","timestamp":1722655512421,"user_tz":240,"elapsed":208,"user":{"displayName":"Max Liu","userId":"01627597745768120009"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["class GFlowNet(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(GFlowNet, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(2, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","        )\n","        self.fwp = nn.Linear(hidden_size, 3)\n","        self.bwp = nn.Linear(hidden_size, 3)\n","        self.log_Z = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, state):\n","        state = self.network(state)\n","        return F.softplus(self.fwp(state)), F.softplus(self.bwp(state))\n","\n","def generate_trajectory(env, model, max_steps):\n","    state = env.reset()\n","    trajectory = [state]\n","    forward_probs = []\n","    actions = []\n","\n","    for _ in range(max_steps):\n","        state_tensor = torch.FloatTensor(state)\n","        # print(state_tensor)\n","        flow_values, _ = model(state_tensor)\n","        probs = flow_values / flow_values.sum()\n","        action = torch.multinomial(probs, 1).item()\n","        # print(flow_values)\n","        # action = torch.distributions.categorical.Categorical(logits=flow_values).sample()\n","        # print(action)\n","        next_state, reward, done = env.step(action)\n","        state = next_state\n","\n","        forward_probs.append(probs[action])\n","        actions.append(action)\n","        trajectory.append(next_state)\n","\n","        if done:\n","            break\n","\n","    return trajectory, forward_probs, actions, reward\n","\n","def calculate_backward_probs(model, trajectory, actions):\n","    backward_probs = []\n","    for t in range(1, len(trajectory)):\n","        prev_state = trajectory[t-1]\n","        curr_state = trajectory[t]\n","\n","        _, backward_flow_values = model(torch.FloatTensor(curr_state))\n","        if curr_state[0] == curr_state[1]:\n","            backward_flow_values[0] = 0\n","            backward_flow_values[1] = 0\n","        if curr_state[0] == curr_state[1] + 1:\n","            backward_flow_values[0] = 0\n","        if curr_state[0] == -curr_state[1]:\n","            backward_flow_values[1] = 0\n","            backward_flow_values[2] = 0\n","        if curr_state[0] == -curr_state[1] + 1:\n","            backward_flow_values[2] = 0\n","        prev_probs = backward_flow_values / backward_flow_values.sum()\n","\n","        # Calculate the action that would lead from prev_state to curr_state\n","        dx = curr_state[0] - prev_state[0]  # Should always be 1\n","        dy = curr_state[1] - prev_state[1]\n","        backward_action = dy + 1  # Map (-1, 0, 1) to (0, 1, 2)\n","\n","        backward_probs.append(prev_probs[backward_action])\n","\n","    return backward_probs\n","\n","def trajectory_balance_loss(model, trajectory, forward_probs, backward_probs, reward):\n","    forward_log_prob = torch.sum(torch.log(torch.stack(forward_probs)))\n","    backward_log_prob = torch.sum(torch.log(torch.stack(backward_probs))) if backward_probs else torch.tensor(0.0)\n","\n","    log_ratio = model.log_Z + forward_log_prob - torch.log(torch.tensor(reward + 1e-10)) - backward_log_prob\n","\n","    loss = log_ratio ** 2\n","    return loss\n","\n","def train_gflownet(num_episodes, max_steps, hidden_size, learning_rate):\n","    env = GridEnvironment(max_steps)\n","    model = GFlowNet(hidden_size)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_episodes)\n","\n","    for episode in range(num_episodes):\n","        trajectory, forward_probs, actions, reward = generate_trajectory(env, model, max_steps)\n","        backward_probs = calculate_backward_probs(model, trajectory, actions)\n","        loss = trajectory_balance_loss(model, trajectory, forward_probs, backward_probs, reward)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        if (episode + 1) % 1000 == 0:\n","            print(f\"Episode {episode + 1}, Loss: {loss.item():.4f}, Reward: {reward}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","    return model"],"metadata":{"id":"mCzSIrgblKSG","executionInfo":{"status":"ok","timestamp":1722656512861,"user_tz":240,"elapsed":195,"user":{"displayName":"Max Liu","userId":"01627597745768120009"}}},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"Io-r0cN8g-uX"}},{"cell_type":"code","source":["# Training parameters\n","num_episodes = 20000  # Increased number of episodes\n","max_steps = 5\n","hidden_size = 32\n","learning_rate = 0.001\n","\n","trained_model = train_gflownet(num_episodes, max_steps, hidden_size, learning_rate)\n","\n","# Generate sample trajectories using the trained model\n","env = GridEnvironment(max_steps)\n","num_samples = 1000  # Increased number of samples\n","sample_trajectories = [generate_trajectory(env, trained_model, max_steps)[0] for _ in range(num_samples)]\n","\n","# Analyze the distribution of end states\n","end_states = [trajectory[-1][1] for trajectory in sample_trajectories]\n","unique, counts = np.unique(end_states, return_counts=True)\n","for y, count in zip(unique, counts):\n","    print(f\"End state y={y}: {count/num_samples:.2%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRe1YYUKlIq7","executionInfo":{"status":"ok","timestamp":1722656693839,"user_tz":240,"elapsed":180785,"user":{"displayName":"Max Liu","userId":"01627597745768120009"}},"outputId":"b30c2eaa-64ec-4e73-eb23-464073992f6b"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1000, Loss: 2.5397, Reward: 4, LR: 0.000994\n","Episode 2000, Loss: 0.1439, Reward: 3, LR: 0.000976\n","Episode 3000, Loss: 0.1668, Reward: 5, LR: 0.000946\n","Episode 4000, Loss: 0.3631, Reward: 3, LR: 0.000905\n","Episode 5000, Loss: 0.4042, Reward: 5, LR: 0.000854\n","Episode 6000, Loss: 0.0863, Reward: 2, LR: 0.000794\n","Episode 7000, Loss: 0.0001, Reward: 5, LR: 0.000727\n","Episode 8000, Loss: 0.0002, Reward: 4, LR: 0.000655\n","Episode 9000, Loss: 0.0041, Reward: 5, LR: 0.000578\n","Episode 10000, Loss: 0.0071, Reward: 3, LR: 0.000500\n","Episode 11000, Loss: 0.0939, Reward: 2, LR: 0.000422\n","Episode 12000, Loss: 0.0059, Reward: 5, LR: 0.000345\n","Episode 13000, Loss: 0.0034, Reward: 5, LR: 0.000273\n","Episode 14000, Loss: 0.0206, Reward: 2, LR: 0.000206\n","Episode 15000, Loss: 0.0000, Reward: 4, LR: 0.000146\n","Episode 16000, Loss: 0.0000, Reward: 4, LR: 0.000095\n","Episode 17000, Loss: 0.0054, Reward: 4, LR: 0.000054\n","Episode 18000, Loss: 0.0183, Reward: 2, LR: 0.000024\n","Episode 19000, Loss: 0.0002, Reward: 5, LR: 0.000006\n","Episode 20000, Loss: 0.0000, Reward: 5, LR: 0.000000\n","End state y=-5: 15.50%\n","End state y=-4: 13.00%\n","End state y=-3: 10.60%\n","End state y=-2: 4.80%\n","End state y=-1: 3.00%\n","End state y=0: 0.70%\n","End state y=1: 2.60%\n","End state y=2: 6.20%\n","End state y=3: 11.50%\n","End state y=4: 12.80%\n","End state y=5: 19.30%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rLVJXaY8Ty4q"},"execution_count":null,"outputs":[]}]}