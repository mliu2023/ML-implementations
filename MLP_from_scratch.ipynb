{"cells":[{"cell_type":"markdown","source":["### MLP from scratch\n","This notebook implements multilayer perceptrons from scratch using Numpy. The networks are tested on the MNIST digit dataset and achieve 99% accuracy."],"metadata":{"id":"fYut2DoSQ1ec"}},{"cell_type":"markdown","metadata":{"id":"U432VdhSYMWI"},"source":["### Importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCzOZKGQYAOv"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"zu1aFa-HZS7L"},"source":["### Setting up datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11566,"status":"ok","timestamp":1714521237329,"user":{"displayName":"Max Liu","userId":"01627597745768120009"},"user_tz":240},"id":"CeuIhLV_ZT_v","outputId":"7a2d328d-a1d2-4814-e788-9a1fe277b29f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}],"source":["import keras\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oy-uPnIh_0zB"},"outputs":[],"source":["x_train_encoded = np.reshape(x_train, (x_train.shape[0], 784)) / 255\n","x_test_encoded = np.reshape(x_test, (x_test.shape[0], 784)) / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOZWNIgI9r2x"},"outputs":[],"source":["y_train_encoded = np.zeros((y_train.shape[0], 10))\n","y_train_encoded[np.arange(y_train.shape[0]), y_train] = 1\n","\n","y_test_encoded = np.zeros((y_test.shape[0], 10))\n","y_test_encoded[np.arange(y_test.shape[0]), y_test] = 1"]},{"cell_type":"markdown","metadata":{"id":"8FsKtfMiviyU"},"source":["### Building the neural network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdTxbVaCvmrc"},"outputs":[],"source":["class Linear:\n","  def __init__(self, in_dim: int, out_dim: int, lr: float) -> None:\n","    self.weights = np.random.normal(0, np.sqrt(2/in_dim), (in_dim, out_dim))\n","    self.biases = np.random.normal(0, np.sqrt(2/in_dim), (1, out_dim))\n","    self.lr = lr\n","\n","  def forward(self, x: np.ndarray) -> np.ndarray:\n","    self.input = x\n","    return np.dot(x, self.weights) + self.biases\n","\n","  def backward(self, grad: np.ndarray) -> np.ndarray:\n","    input_grad = np.dot(grad, self.weights.T)\n","    weights_grad = np.dot(self.input.T, grad)\n","    self.weights -= weights_grad * self.lr / grad.shape[0]\n","    self.biases -= np.sum(grad, axis=0) * self.lr / grad.shape[0]\n","    return input_grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoCdcCtWw3H5"},"outputs":[],"source":["class ReLU:\n","  def __init__(self):\n","    pass\n","\n","  def forward(self, x: np.ndarray) -> np.ndarray:\n","    self.input = x\n","    return np.maximum(x, np.zeros(x.shape))\n","\n","  def backward(self, grad: np.ndarray) -> np.ndarray:\n","    return (self.input > 0).astype(np.float32) * grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7joondDoybXf"},"outputs":[],"source":["class Softmax:\n","  def __init__(self):\n","    pass\n","\n","  def forward(self, x: np.ndarray) -> np.ndarray:\n","    e = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    self.probs = e / np.sum(e, axis=1, keepdims=True)\n","    return self.probs\n","\n","  def backward(self, grad: np.ndarray) -> np.ndarray:\n","    self.backprop = np.zeros((self.probs.shape[0], self.probs.shape[1]))\n","    for i in range(self.probs.shape[0]):\n","      self.backprop[i] = np.dot(grad[i], (np.diag(self.probs[i]) - np.outer(self.probs[i], self.probs[i])))\n","      # self.backprop[i] = np.sum((np.diag(self.probs[i]) - np.outer(self.probs[i], self.probs[i])) * grad[i], axis=1).T\n","    # self.backprop = np.diag(np.sum(self.probs, axis=0)) - np.outer(np.sum(self.probs, axis=0), np.sum(self.probs, axis=0))\n","\n","    # x = np.sum(self.backprop * np.sum(grad, axis=0), axis=1, keepdims=True).T\n","\n","    # return x\n","    # print(self.backprop)\n","    return self.backprop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGN_nq6su9Mc"},"outputs":[],"source":["class CrossEntropyLoss:\n","  def __init__(self):\n","    self.EPSILON = 1e-15\n","\n","  def forward(self, pred_prob, true_prob) -> np.ndarray:\n","    return -1/pred_prob.shape[0] * true_prob * np.log(pred_prob + self.EPSILON)\n","\n","  def backward(self, pred_prob, true_prob) -> np.ndarray:\n","    return -1/pred_prob.shape[0] * true_prob / (pred_prob + self.EPSILON)"]},{"cell_type":"markdown","metadata":{"id":"6rN-EIZ79A5a"},"source":["### Training the neural network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1h_JXOA9CP7"},"outputs":[],"source":["lr = 1e-3\n","batch_size = 3\n","verbose = False\n","\n","layer1 = Linear(784, 100, lr)\n","layer2 = Linear(100, 50, lr)\n","layer3 = Linear(50, 10, lr)\n","softmax = Softmax()\n","relu1 = ReLU()\n","relu2 = ReLU()\n","ce = CrossEntropyLoss()\n","\n","for epoch in range(100):\n","  correct = 0\n","  incorrect = x_train.shape[0]\n","  total_loss = 0\n","\n","  for i in range(0, x_train.shape[0], batch_size):\n","    x = x_train_encoded[i:i+batch_size]\n","    if verbose: print(f\"x shape: {x.shape}\")\n","    y = y_train_encoded[i:i+batch_size]\n","    if verbose: print(f\"y shape: {y.shape}\")\n","\n","    x = layer1.forward(x)\n","    if verbose: print(f\"layer 1 output: {x.shape}\")\n","    x = relu1.forward(x)\n","    if verbose: print(f\"relu 1 output: {x.shape}\")\n","    x = layer2.forward(x)\n","    if verbose: print(f\"layer 2 output: {x.shape}\")\n","    x = relu2.forward(x)\n","    if verbose: print(f\"relu 2 output: {x.shape}\")\n","    x = layer3.forward(x)\n","    if verbose: print(f\"layer 3 output: {x.shape}\")\n","    x = softmax.forward(x)\n","    if verbose: print(f\"softmax output: {x.shape}\")\n","\n","    y_pred = np.argmax(x, axis=1)\n","    if verbose: print(f\"pred argmax output: {y_pred.shape}\")\n","    y_true = np.argmax(y, axis=1)\n","    if verbose: print(f\"true argmax output: {y_true.shape}\")\n","    correct += np.sum(y_pred == y_true)\n","\n","    total_loss += np.sum(ce.forward(x, y))\n","\n","    grad = ce.backward(x, y)\n","    if verbose: print(f\"ce grad: {grad.shape}\")\n","    grad = softmax.backward(grad)\n","    if verbose: print(f\"softmax grad: {grad.shape}\")\n","    grad = layer3.backward(grad)\n","    if verbose: print(f\"layer 3 grad: {grad.shape}\")\n","    grad = relu2.backward(grad)\n","    if verbose: print(f\"relu 2 grad: {grad.shape}\")\n","    grad = layer2.backward(grad)\n","    if verbose: print(f\"layer 2 grad: {grad.shape}\")\n","    grad = relu1.backward(grad)\n","    if verbose: print(f\"relu 1 grad: {grad.shape}\")\n","    grad = layer1.backward(grad)\n","    if verbose: print(f\"layer 1 grad: {grad.shape}\")\n","\n","  print(f\"average loss: {total_loss/x_train.shape[0]}\")\n","  print(f\"accuracy: {correct/incorrect}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNgxDVLdH-iT"},"outputs":[],"source":["correct = 0\n","incorrect = x_test.shape[0]\n","total_loss = 0\n","\n","for i in range(x_test.shape[0]):\n","  x = x_test_encoded[i]\n","  y = y_test_encoded[i]\n","\n","  x = layer1.forward(x)\n","  x = relu1.forward(x)\n","  x = layer2.forward(x)\n","  x = relu2.forward(x)\n","  x = layer3.forward(x)\n","  x = softmax.forward(x)\n","\n","  y_pred = np.argmax(x)\n","  y_true = np.argmax(y)\n","  correct += np.sum(y_pred == y_true, axis=0)\n","\n","  total_loss += np.sum(ce.forward(x, y))\n","\n","print(f\"accuracy: {correct/incorrect}\")\n","print(f\"total loss: {total_loss}\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjllkxCzosWzR1fBCJDk1v"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}